{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests, os, pandas\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "AA_table = {\r\n",
    "    \"Ala\": \"A\", \"Arg\": \"R\", \"Asn\": \"N\", \"Asp\": \"D\", \"Cys\": \"C\", \"Gln\": \"Q\",\r\n",
    "    \"Glu\": \"E\", \"Gly\": \"G\", \"His\": \"H\", \"Ile\": \"I\", \"Leu\": \"L\", \"Lys\": \"K\",\r\n",
    "    \"Met\": \"M\", \"Phe\": \"F\", \"Pro\": \"P\", \"Pyl\": \"O\", \"Ser\": \"S\", \"Sec\": \"U\",\r\n",
    "    \"Thr\": \"T\", \"Trp\": \"W\", \"Tyr\": \"Y\", \"Val\": \"V\", \"Asx\": \"B\", \"Glx\": \"Z\",\r\n",
    "    \"Xaa\": \"X\", \"Xle\": \"J\", \"-\" : \"*\"\r\n",
    "}\r\n",
    " \r\n",
    "def get_CleavageSite(name) :\r\n",
    "\r\n",
    "    if os.path.isfile(os.path.join(os.path.abspath('./'), 'CleavageSite', name + '_Site.csv')) :\r\n",
    "        return \r\n",
    "\r\n",
    "    print('get cleavage site %s' % (name))\r\n",
    "\r\n",
    "    url = 'https://www.ebi.ac.uk/merops/cgi-bin/substrates?id=' + name\r\n",
    "\r\n",
    "    r = requests.get(url)\r\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\r\n",
    "\r\n",
    "    table = soup.find_all('table')[0]\r\n",
    "\r\n",
    "    iter_rows = iter(table.find_all('tr'))\r\n",
    "    next(iter_rows)\r\n",
    "\r\n",
    "    Site = []\r\n",
    "\r\n",
    "    for row in iter_rows :\r\n",
    "        \r\n",
    "        columns = row.find_all('td')\r\n",
    "        \r\n",
    "        CleavageSite = columns[3].get_text()\r\n",
    "\r\n",
    "        line = []\r\n",
    "        line.append(str(columns[0].get_text()))\r\n",
    "\r\n",
    "        # get amino acids\r\n",
    "        for i in range(6, 14) :\r\n",
    "            amino_acid = str(columns[i].get_text())\r\n",
    "            if AA_table.get(amino_acid) == None :\r\n",
    "                line.append(amino_acid)\r\n",
    "            else :\r\n",
    "                line.append(AA_table[amino_acid])\r\n",
    "\r\n",
    "\r\n",
    "        # print(line)\r\n",
    "        Site.append(line)\r\n",
    "    \r\n",
    "    df = pandas.DataFrame(Site, columns = ['name', 'P4', 'P3', 'P2', 'P1', 'P1\\'', 'P2\\'', 'P3\\'', 'P4\\''])\r\n",
    "    df.to_csv(os.path.join(os.path.abspath('./'), 'CleavageSite', name + '_Site' + '.csv'), index = False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests, os, pandas, re\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "\r\n",
    "def get_sequence(id) :\r\n",
    "\r\n",
    "    print('get %s' % (id))\r\n",
    "\r\n",
    "    url = 'https://www.ebi.ac.uk/merops/cgi-bin/aaseq?mernum=' + id\r\n",
    "\r\n",
    "    r = requests.get(url)\r\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\r\n",
    "\r\n",
    "    content = soup.find_all('pre')\r\n",
    "\r\n",
    "    Sequence = ''\r\n",
    "    text = str(content[0].text)\r\n",
    "\r\n",
    "    return re.sub('[^A-Z]', '', text[text.find('1        ') :])\r\n",
    "\r\n",
    "\r\n",
    "def get_protease(name) :\r\n",
    "    \r\n",
    "    if os.path.isfile(os.path.join(os.path.abspath('./'), 'Protease', name + '_Protease.csv')) :\r\n",
    "        return \r\n",
    "\r\n",
    "    print('get protease %s' % (name))\r\n",
    "        \r\n",
    "    url = 'https://www.ebi.ac.uk/merops/cgi-bin/sequence_features?mid=' + name\r\n",
    "    \r\n",
    "    r = requests.get(url)\r\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\r\n",
    "\r\n",
    "    table = soup.find_all('table')[0]\r\n",
    "\r\n",
    "    iter_rows = iter(table.find_all('tr'))\r\n",
    "    next(iter_rows)\r\n",
    "\r\n",
    "    st = set()\r\n",
    "    Sequence = []\r\n",
    "\r\n",
    "    for row in iter_rows :\r\n",
    "        \r\n",
    "        columns = row.find_all('td')\r\n",
    "        \r\n",
    "        # Crawling data\r\n",
    "        length = int(columns[2].get_text())\r\n",
    "        Species = str(columns[1].get_text())\r\n",
    "        sequence_name = str(columns[0].get_text())\r\n",
    "        PeptidaseUnit = str(columns[3].get_text())\r\n",
    "        ActiveSiteResidus = str(columns[4].get_text())\r\n",
    "        sequence = get_sequence(columns[0].get_text())\r\n",
    "\r\n",
    "        # remove duplicate\r\n",
    "        if sequence_name in st : continue\r\n",
    "        st.add(sequence_name)\r\n",
    "\r\n",
    "        # check sequence length\r\n",
    "        if len(sequence) != length :\r\n",
    "            print('WTF ' + sequence_name)\r\n",
    "            continue\r\n",
    "\r\n",
    "        line = [sequence_name, Species, length, PeptidaseUnit, ActiveSiteResidus, sequence]\r\n",
    "\r\n",
    "        # print(line)\r\n",
    "        Sequence.append(line)\r\n",
    "    \r\n",
    "    df = pandas.DataFrame(Sequence, columns = ['MERNUM', 'Species', 'Length', 'Peptidase unit', 'Active site reidues', 'Sequence'])\r\n",
    "    df.to_csv(os.path.join(os.path.abspath('./'), 'Protease', name + '_Protease' + '.csv'), index = False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import concurrent.futures, time\r\n",
    "\r\n",
    "# get protease list\r\n",
    "def get_list(family) :\r\n",
    "    \r\n",
    "    ret = []\r\n",
    "\r\n",
    "    URL = 'https://www.ebi.ac.uk/merops/cgi-bin/peptidase_specificity'\r\n",
    "    \r\n",
    "    # get webcontent from the web\r\n",
    "    r = requests.get(URL)\r\n",
    "\r\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\r\n",
    "    table = soup.find_all('tr', {\"class\": \"alt\"})\r\n",
    "\r\n",
    "    for it in table :\r\n",
    "\r\n",
    "        # get protease ID\r\n",
    "        id = it.find_all('td')[0].text\r\n",
    "\r\n",
    "        if id.startswith(family) :\r\n",
    "            ret.append(id)\r\n",
    "\r\n",
    "    return ret\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# multi thread\r\n",
    "def job(id) :\r\n",
    "    print('Crawling %s.' % (id))\r\n",
    "    get_protease(id)\r\n",
    "    get_CleavageSite(id)\r\n",
    "    print('%s done.' % id)\r\n",
    "\r\n",
    "\r\n",
    "targets = get_list('S08')\r\n",
    "\r\n",
    "\r\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers = 10) as executor :\r\n",
    "    executor.map(job, targets)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "f27f2496960f23253b89e7b8bb5fad291f16c34ca6ee598496faa9251168aae6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}